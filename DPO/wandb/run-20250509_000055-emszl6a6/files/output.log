[2025-05-09 00:00:57,028] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
 29%|██████████████████████████████████████████████████                                                                                                                             | 143/500 [00:49<02:03,  2.88it/s]
epoch 0, step 10, loss 0.7403299808502197, lr 2e-07, grad_norm 136.341274047742
epoch 0, step 20, loss 0.6388451457023621, lr 4e-07, grad_norm 41.78072817774117
epoch 0, step 30, loss 0.6551753282546997, lr 6e-07, grad_norm 43.85244421696268
epoch 0, step 40, loss 1.0327632427215576, lr 8e-07, grad_norm 154.71224089051685
epoch 0, step 50, loss 0.5619210004806519, lr 1e-06, grad_norm 68.28800188117633
epoch 1, step 60, loss 0.1844598650932312, lr 9.98782025129912e-07, grad_norm 69.11685222525263
epoch 1, step 70, loss 0.5233429074287415, lr 9.95134034370785e-07, grad_norm 36.1381989072526
epoch 1, step 80, loss 0.4763437509536743, lr 9.890738003669027e-07, grad_norm 52.222857289586145
epoch 1, step 90, loss 0.6297504305839539, lr 9.806308479691594e-07, grad_norm 84.53076508410605
epoch 1, step 100, loss 0.8382245898246765, lr 9.698463103929541e-07, grad_norm 167.7224765770116
epoch 2, step 110, loss 0.6050766706466675, lr 9.567727288213004e-07, grad_norm 18.520955692147005
epoch 2, step 120, loss 0.1494917869567871, lr 9.414737964294634e-07, grad_norm 41.87324508717035
epoch 2, step 130, loss 0.3987826108932495, lr 9.240240480782129e-07, grad_norm 22.530771368542485
epoch 2, step 140, loss 0.3363746106624603, lr 9.045084971874737e-07, grad_norm 49.274504238219144
Traceback (most recent call last):
  File "/home/tonyzst/Desktop/CS224R-Project/DPO/train.py", line 126, in <module>
    train(model, ref_model, dataloader, optimizer, device, beta, scheduler)
  File "/home/tonyzst/Desktop/CS224R-Project/DPO/train.py", line 55, in train
    prompt_length = batch['attention_mask_prompt'].sum(dim=1).to(device)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/tonyzst/Desktop/CS224R-Project/DPO/train.py", line 126, in <module>
    train(model, ref_model, dataloader, optimizer, device, beta, scheduler)
  File "/home/tonyzst/Desktop/CS224R-Project/DPO/train.py", line 55, in train
    prompt_length = batch['attention_mask_prompt'].sum(dim=1).to(device)
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7f195a016dd0>
Traceback (most recent call last):
  File "/home/tonyzst/anaconda3/envs/cs224r-project/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 94, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/tonyzst/anaconda3/envs/cs224r-project/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 226, in teardown
    self._router.join()
  File "/home/tonyzst/anaconda3/envs/cs224r-project/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/tonyzst/anaconda3/envs/cs224r-project/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/tonyzst/anaconda3/envs/cs224r-project/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
